{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a792d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "model.safetensors:  40%|███▉      | 9.57G/24.2G [12:42<18:51, 12.9MB/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from modify_gptj import modify_gptj\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NbAiLab/nb-gpt-j-6B\")\n",
    "\n",
    "cuda0 = \"cuda:0\" if torch.cuda_is_available() else \"cpu\"\n",
    "cuda1 = \"cuda:1\" if torch.cuda_is_available() else \"cpu\"\n",
    "\n",
    "normal_model = AutoModelForCausalLM.from_pretrained(\"NbAiLab/nb-gpt-j-6B\").to(cuda0)\n",
    "normal_model.eval()\n",
    "\n",
    "pos_shift_model = AutoModelForCausalLM.from_pretrained(\"NbAiLab/nb-gpt-j-6B\").to(cuda1)\n",
    "modify_gptj(pos_shift_model)\n",
    "pos_shift_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .cache_regimes import SlidingWindowCache, AttentionSinkCache\n",
    "\n",
    "# Load the first 10,000 words of the report\n",
    "with open(\"report.txt\") as f:\n",
    "    book_text = f.read()\n",
    "n_words = 10_000\n",
    "text = \" \".join(book_text.split(\" \")[:n_words])\n",
    "\n",
    "# Instantiate different cache regimes\n",
    "GPTJ_CTX_LEN = 2048\n",
    "cache_size = GPTJ_CTX_LEN - 1\n",
    "\n",
    "sliding_window = SlidingWindowCache(cache_size=cache_size)\n",
    "attention_sink = AttentionSinkCache(start_size=4, recent_size=cache_size - 4)\n",
    "attention_sink_no_start = AttentionSinkCache(start_size=0, recent_size=cache_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b8b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .evaluation import evalute_on_text\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "nlls_sw = evalute_on_text(normal_model, tokenizer, text, sliding_window)\n",
    "ppl_sw = torch.exp(nlls_sw.mean())\n",
    "\n",
    "print(f\"PPL for sliding window {ppl_sw:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "nlls_as = evalute_on_text(pos_shift_model, tokenizer, text, attention_sink)\n",
    "ppl_as = torch.exp(nlls_as.mean())\n",
    "\n",
    "print(f\"PPL for attention sink {ppl_as}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "nlls_no_cache = evalute_on_text(normal_model, tokenizer, text)\n",
    "ppl_no_cache = torch.exp(nlls_no_cache.mean())\n",
    "print(f\"PPL without cache {ppl_no_cache}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "nlls_sw_pos_shift = evalute_on_text(pos_shift_model, text, attention_sink_no_start)\n",
    "ppl_sw_pos_shift = torch.exp(nlls_sw_pos_shift.mean())\n",
    "\n",
    "print(f\"PPL for sliding window, recomputing cache {ppl_sw_pos_shift}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .plots import graph_sliding_window_ppl\n",
    "\n",
    "graph_sliding_window_ppl(nlls_sw_pos_shift[:5000], cache_regime = \"Window Attention with Recomputation of Positional Embeddins\", window_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .plots import graph_attentions\n",
    "\n",
    "n_sentences = 100\n",
    "n_tokens = 20\n",
    "\n",
    "# Get the first n_sentences sentences with n_tokens tokens\n",
    "sentences = list(\n",
    "    filter(lambda s: len(tokenizer(s).input_ids) == n_tokens, book_text.split(\".\"))\n",
    ")[:n_sentences]\n",
    "\n",
    "# Get attentions for each sentence\n",
    "attns = []\n",
    "for sentence in sentences:\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids.to(\n",
    "            normal_model.device\n",
    "        )\n",
    "        outputs = normal_model(input_ids, output_attentions=True)\n",
    "    attns.append(torch.stack(outputs.attentions, dim=0))\n",
    "\n",
    "sentence_attns = torch.stack(attns).mean(0).squeeze()[:, 0, :, :].to(\"cpu\")\n",
    "graph_attentions(sentence_attns, head_idxs=[0, 1, 5, 16], n_cols=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
